{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Fingerspelling Recognizer and Predictor\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import mediapipe\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.transform import resize\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = os.environ.get('SIS_DATASET_PATH')\n",
    "dsdf = pd.read_csv(os.path.join(ds_path, 'train.csv')) # Modify path to match kaggle dataset path\n",
    "print(\"Dataset Shape: {}\".format(dsdf.shape))\n",
    "\n",
    "dsdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset from Parquets to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is taken from the Google example notebook for processing data\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
    "\n",
    "FEATURE_COLUMNS = X + Y + Z\n",
    "\n",
    "X_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"x_\" in col]\n",
    "Y_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"y_\" in col]\n",
    "Z_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"z_\" in col]\n",
    "\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tfrec_path = os.path.join(ds_path, 'tfrecords')\n",
    "\n",
    "# Set length of frames to 128\n",
    "FRAME_LEN = 128\n",
    "\n",
    "# Create directory to store the new data\n",
    "if not os.path.isdir(\"preprocessed\"):\n",
    "    os.mkdir(\"preprocessed\")\n",
    "else:\n",
    "    shutil.rmtree(\"preprocessed\")\n",
    "    os.mkdir(\"preprocessed\")\n",
    "\n",
    "# Loop through each file_id\n",
    "for file_id in tqdm(dsdf.file_id.unique()):\n",
    "    # Parquet file name\n",
    "    \n",
    "    # pq_file = f\"/kaggle/input/asl-fingerspelling/train_landmarks/{file_id}.parquet\"\n",
    "    pq_file = os.path.join(ds_path, 'train_landmarks', f'{file_id}.parquet')\n",
    "\n",
    "    # Filter train.csv and fetch entries only for the relevant file_id\n",
    "    file_df = dsdf.loc[dsdf[\"file_id\"] == file_id]\n",
    "    # Fetch the parquet file\n",
    "    parquet_df = pq.read_table(pq_file,\n",
    "                              columns=['sequence_id'] + FEATURE_COLUMNS).to_pandas()\n",
    "    # File name for the updated data\n",
    "    tf_file = os.path.join(tfrec_path, f\"{file_id}.tfrec\")\n",
    "    parquet_numpy = parquet_df.to_numpy()\n",
    "    # Initialize the pointer to write the output of \n",
    "    # each `for loop` below as a sequence into the file.\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        # Loop through each sequence in file.\n",
    "        for seq_id, phrase in zip(file_df.sequence_id, file_df.phrase):\n",
    "            # Fetch sequence data\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "            \n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "            \n",
    "            if 2*len(phrase)<no_nan:\n",
    "                features = {FEATURE_COLUMNS[i]: tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(FEATURE_COLUMNS))}\n",
    "                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')]))\n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
    "                file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_records = dsdf.file_id.map(lambda x: os.path.join(tfrec_path, f'{x}.tfrecord')).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sis_cv_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

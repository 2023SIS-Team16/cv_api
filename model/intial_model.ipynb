{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Fingerspelling Recognizer and Predictor\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import mediapipe\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.transform import resize\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = os.environ.get('SIS_DATASET_INPUT_PATH')\n",
    "ds_out_path = os.environ.get('SIS_DATASET_PREPROC_PATH')\n",
    "dsdf = pd.read_csv(os.path.join(ds_path, 'train.csv')) # Modify path to match kaggle dataset path\n",
    "print(\"Dataset Shape: {}\".format(dsdf.shape))\n",
    "\n",
    "dsdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset from Parquets to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is taken from the Google example notebook for processing data\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
    "\n",
    "FEATURE_COLUMNS = X + Y + Z\n",
    "\n",
    "X_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"x_\" in col]\n",
    "Y_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"y_\" in col]\n",
    "Z_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"z_\" in col]\n",
    "\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tfrec_path = os.path.join(ds_out_path, 'tfrecords')\n",
    "\n",
    "# Set length of frames to 128\n",
    "FRAME_LEN = 128\n",
    "\n",
    "# Create directory to store the new data\n",
    "if not os.path.isdir(\"preprocessed\"):\n",
    "    os.mkdir(\"preprocessed\")\n",
    "else:\n",
    "    shutil.rmtree(\"preprocessed\")\n",
    "    os.mkdir(\"preprocessed\")\n",
    "\n",
    "# Loop through each file_id\n",
    "for file_id in tqdm(dsdf.file_id.unique()):\n",
    "    # Parquet file name\n",
    "    \n",
    "    # pq_file = f\"/kaggle/input/asl-fingerspelling/train_landmarks/{file_id}.parquet\"\n",
    "    pq_file = os.path.join(ds_path, 'train_landmarks', f'{file_id}.parquet')\n",
    "\n",
    "    # Filter train.csv and fetch entries only for the relevant file_id\n",
    "    file_df = dsdf.loc[dsdf[\"file_id\"] == file_id]\n",
    "    # Fetch the parquet file\n",
    "    parquet_df = pq.read_table(pq_file,\n",
    "                              columns=['sequence_id'] + FEATURE_COLUMNS).to_pandas()\n",
    "    # File name for the updated data\n",
    "    tf_file = os.path.join(tfrec_path, f\"{file_id}.tfrec\")\n",
    "    parquet_numpy = parquet_df.to_numpy()\n",
    "    # Initialize the pointer to write the output of \n",
    "    # each `for loop` below as a sequence into the file.\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        # Loop through each sequence in file.\n",
    "        for seq_id, phrase in zip(file_df.sequence_id, file_df.phrase):\n",
    "            # Fetch sequence data\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "            \n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "            \n",
    "            if 2*len(phrase)<no_nan:\n",
    "                features = {FEATURE_COLUMNS[i]: tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(FEATURE_COLUMNS))}\n",
    "                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')]))\n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
    "                file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_records = dsdf.file_id.map(lambda x: os.path.join(tfrec_path, f'{x}.tfrecord')).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (os.path.join(ds_path, 'character_to_prediction_index.json'), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "# Token definitions\n",
    "pad_token = 'P'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token_index = 59\n",
    "start_token_index = 60\n",
    "end_token_index = 61\n",
    "\n",
    "char_to_num[pad_token] = pad_token_index\n",
    "char_to_num[start_token] = start_token_index\n",
    "char_to_num[end_token] = end_token_index\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was taken from: https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "\n",
    "# Function to resize and add padding.\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "# Detect the dominant hand from the number of NaN values.\n",
    "# Dominant hand will have less NaN values since it is in frame moving.\n",
    "def pre_process(x):\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    \n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    \n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # For dominant hand\n",
    "    if rnans > lnans:\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        \n",
    "        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n",
    "        \n",
    "        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n",
    "    else:\n",
    "        hand = rhand\n",
    "        pose = rpose\n",
    "    \n",
    "    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n",
    "    \n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "\n",
    "    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "    \n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    x = resize_pad(x)\n",
    "    \n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_func(record_bytes):\n",
    "    schema = {col: tf.io.VarLenFeature(dtype=tf.float32) for col in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase\n",
    "\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(list(char_to_num.keys())),\n",
    "        values=tf.constant(list(char_to_num.values())),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def convert_func(landmarks, phrase):\n",
    "    phrase = start_token + phrase + end_token # Set up phrase\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "\n",
    "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode='CONSTANT', constant_values=pad_token_index)\n",
    "    return pre_process(landmarks), phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Split\n",
    "batch_size = 128\n",
    "train_len = int(0.8 * len(tf_records))\n",
    "train_ds = tf.data.TFRecordDataset(tf_records[:train_len]).map(decode_func).map(convert_func).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sis_cv_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
